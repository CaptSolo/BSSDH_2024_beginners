{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - NLTK - Continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Corpora and Lexical Resources\n",
    "\n",
    "based on the NLTK book:\n",
    "\n",
    "[\"Accessing Text Corpora and Lexical Resources\"](https://www.nltk.org/book/ch02.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Text Corpora\n",
    "\n",
    "NLTK includes many text collections (corpora) and other language resources, listed here: http://www.nltk.org/nltk_data/\n",
    "\n",
    "Additional information:\n",
    "* [NLTK Corpus How-to](http://www.nltk.org/howto/corpus.html)\n",
    "\n",
    "In order to use these resources you may need to download them using `nltk.download()`\n",
    "\n",
    "---\n",
    "\n",
    "**NLTK book: [\"Text Corpus Structure\"](https://www.nltk.org/book/ch02#text-corpus-structure)**\n",
    "\n",
    "There are different types of corpora:\n",
    "* simple collections of text (e.g. Gutenberg corpus)\n",
    "* categorized (texts are grouped into categories that might correspond to genre, source, author)\n",
    "* temporal, demonstrating language use over a time period (e.g. news texts)\n",
    "\n",
    "![Types of NLTK corpora](https://www.nltk.org/images/text-corpus-structure.png)\n",
    "\n",
    "There are also annotated text corpora that contain linguistic annotations, representing POS tags, named entities, semantic roles, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Gutenberg Corpus\n",
    "\n",
    "NLTK includes a small selection of texts (= multiple files) from the Project Gutenberg electronic text archive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore its contents:\n",
    "\n",
    "nltk.corpus.gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Emma\" by Jane Austen\n",
    "\n",
    "emma = nltk.corpus.gutenberg.words('austen-emma.txt')\n",
    "\n",
    "print(emma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can access corpus texts as characters, words (tokens) or sentences:\n",
    "\n",
    "file_id = 'austen-emma.txt'\n",
    "\n",
    "print(\"\\nSentences:\")\n",
    "print( nltk.corpus.gutenberg.sents(file_id)[:3] )\n",
    "\n",
    "print(\"\\nWords:\")\n",
    "print( nltk.corpus.gutenberg.words(file_id)[:10] )\n",
    "\n",
    "print(\"\\nChars:\")\n",
    "print( nltk.corpus.gutenberg.raw(file_id)[:50] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://www.nltk.org/book/ch02#gutenberg-corpus on how to compute statistics of words, sentences and characters (e.g. avg words per sentence).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Brown corpus\n",
    "\n",
    "The Brown Corpus was the first million-word electronic corpus of English, created in 1961 at Brown University.\n",
    "\n",
    "This corpus contains text from 500 sources, and the sources have been categorized by genre, such as news, editorial, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brown corpus categories list:\n",
    "\n",
    "from nltk.corpus import brown\n",
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter the corpus by (a) one or more categories or (b) file IDs:\n",
    "\n",
    "print(brown.sents(categories='science_fiction')[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in brown.sents(categories='science_fiction')[:2]:\n",
    "    print(\" \".join(sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.sents(categories=['news', 'editorial', 'reviews']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in brown.sents(categories=['news', 'editorial', 'reviews'])[:3]:\n",
    "    print(\" \".join(sent))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(brown.words(fileids=['cg22']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use NLTK **ConditionalFreqDist** to collect statistics on the corpus distribution across genres and other properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfd = nltk.ConditionalFreqDist(\n",
    "           (genre, word)\n",
    "           for genre in brown.categories()\n",
    "           for word in brown.words(categories=genre))\n",
    "\n",
    "genres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\n",
    "modals = ['can', 'could', 'may', 'might', 'must', 'will']\n",
    "\n",
    "cfd.tabulate(conditions=genres, samples=modals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brown corpus contains tags with part-of-speech information\n",
    "\n",
    "[Working with Tagged Corpora](https://www.nltk.org/book/ch05#tagged-corpora) (NLTK book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = nltk.corpus.brown.tagged_words(tagset='universal')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# islice() lets us read a part of the corpus\n",
    "\n",
    "from itertools import islice\n",
    "words = islice(words, 300)\n",
    "\n",
    "# let's convert it to a list\n",
    "word_list = list(words)\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all words with POS tag \"ADJ\"\n",
    "\n",
    "tag = 'ADJ'\n",
    "\n",
    "[item[0] for item in word_list if item[1] == tag]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Additional examples** (using FreqDist, ...):\n",
    "    \n",
    "[Working with Tagged Corpora](https://www.nltk.org/book/ch05#tagged-corpora)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) NLTK Corpus functionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* fileids()  = the files of the corpus\n",
    "* fileids([categories])  = the files of the corpus corresponding to these categories\n",
    "\n",
    "* categories()  = the categories of the corpus\n",
    "* categories([fileids])  = the categories of the corpus corresponding to these files\n",
    "\n",
    "* raw()  = the raw content of the corpus\n",
    "* raw(fileids=[f1,f2,f3])  = the raw content of the specified files\n",
    "* raw(categories=[c1,c2])  = the raw content of the specified categories\n",
    "\n",
    "* words()  = the words of the whole corpus\n",
    "* words(fileids=[f1,f2,f3])  = the words of the specified fileids\n",
    "* words(categories=[c1,c2])  = the words of the specified categories\n",
    "\n",
    "* sents()  = the sentences of the whole corpus\n",
    "* sents(fileids=[f1,f2,f3])  = the sentences of the specified fileids\n",
    "* sents(categories=[c1,c2])  = the sentences of the specified categories\n",
    "\n",
    "* abspath(fileid)  = the location of the given file on disk\n",
    "* encoding(fileid)  = the encoding of the file (if known)\n",
    "* open(fileid)  = open a stream for reading the given corpus file\n",
    "* root  = if the path to the root of locally installed corpus\n",
    "\n",
    "* readme()  = the contents of the README file of the corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: if you want to explore these corpora using `nltk.Text` functionality (e.g. as in the Introduction part) you will need to load them into `nltk.Text`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexical Resources\n",
    "\n",
    "A lexicon, or lexical resource, is a collection of words and/or phrases along with associated information such as part of speech and sense definitions.\n",
    "\n",
    "https://www.nltk.org/book/ch02#lexical-resources\n",
    "\n",
    "We already used NLTK lexical resources (stopwords and common English words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordNet\n",
    "\n",
    "WordNet is a semantically-oriented dictionary of English, similar to a traditional thesaurus but with a richer structure. NLTK includes the English WordNet, with 155,287 words and 117,659 synonym sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a collection of synonym sets related to \"wind\"\n",
    "\n",
    "wn.synsets('wind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words (lemmas) in one of synsets:\n",
    "\n",
    "wn.synset('wind.n.08').lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('wind.n.08').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wn.synset('wind.n.08').examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's explore all the synsets for this word\n",
    "\n",
    "for synset in wn.synsets('wind'):\n",
    "    print(synset.lemma_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all synsets that contain a given word\n",
    "\n",
    "wn.lemmas('curve')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Additional WordNet examples:**\n",
    "* https://www.nltk.org/book/ch02#wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try it yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing with Language: Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# make sure that NLTK language resources have been downloaded \n",
    "# (see \"NLTK Introduction\" notebook)\n",
    "\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frequency distribution of text1\n",
    "fdist1 = FreqDist(text1)\n",
    "\n",
    "print(fdist1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Word] Frequency Distributions\n",
    "\n",
    "FreqDist is used to encode \"frequency distributions\", which count the number of times that each outcome of an experiment occurs.\n",
    "* In case of text, its frequency distribution will contains counts of all tokens that appear in the text.\n",
    "* Technically: FreqDist() creates a Python object (that holds information about a frequency distribution)\n",
    "\n",
    "**FreqDist** methods:\n",
    "\n",
    "* freq(sample) - returns the number of times \"sample\" appears in FreqDist\n",
    "* hapaxes() - a list of samples that appear only once\n",
    "* max() - the sample with the maximum number of occurences\n",
    "* plot() - plot a FreqDist chart\n",
    "* pprint() - \"pretty print\" the first items of FreqDist\n",
    "\n",
    "NLTK book: http://www.nltk.org/book/ch01.html#computing-with-language-simple-statistics\n",
    "\n",
    "Full list of methods: http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print frequency distribution (top results)\n",
    "\n",
    "fdist1.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max()\n",
    "\n",
    "fdist1.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq()\n",
    "\n",
    "print(\"','  :\", fdist1.freq(\",\"))\n",
    "print(\"whale:\", fdist1.freq(\"whale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Information about Python dictionaries: \n",
    "* [\"Dictionaries and Structuring Data\"](https://automatetheboringstuff.com/chapter5/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of fdist1.pprint() looks like a Python \"dictionary\"\n",
    "\n",
    "# can we look up its values by a given \"key\"?\n",
    "fdist1[\"whale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 results (not that interesting for text)\n",
    "\n",
    "fdist1.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most_common() returns a list -> we can \"slice\" it\n",
    "\n",
    "my_list = fdist1.most_common(100)\n",
    "\n",
    "# results 50 through 59\n",
    "my_list[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution\n",
    "fdist1.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# least common results (first 10 examples)\n",
    "\n",
    "fdist1.hapaxes()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words can appear both in lowercase and Capitalized\n",
    "\n",
    "Let's fix our FreqDist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to \"lowercase\" the text before passing it to FreqDist\n",
    "#   - see example in https://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "\n",
    "fdist2 = FreqDist(word.lower() for word in text1)\n",
    "\n",
    "# we're going through the list of tokens in text,\n",
    "#  - returning (generating) lowercase versions of these tokens\n",
    "#  - and passing the result to FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial:\n",
    "print(fdist1.freq(\"whale\"))\n",
    "print(fdist1.freq(\"Whale\"))\n",
    "print()\n",
    "\n",
    "# fixed:\n",
    "print(fdist2.freq(\"whale\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data: removing stopwords\n",
    "\n",
    "NLTK contains a corpus of *stopwords* - high-frequency words like \"the\", \"to\" and \"also\" - that we may want to filter out of a document before further processing.\n",
    "\n",
    "Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts.\n",
    "\n",
    "https://www.nltk.org/book/ch02#wordlist-corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# English stopwords\n",
    "stop_words = stopwords.words(\"english\")\n",
    "\n",
    "stop_words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's start with text1 in lowercase\n",
    "\n",
    "# return a list  \n",
    "#   containing \"word.lower()\"\n",
    "#     for every item (stored in variable \"word\")\n",
    "#       in resource \"text1\"\n",
    "\n",
    "text = [word.lower() for word in text1]\n",
    "\n",
    "text[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK book: [4.2   Operating on Every Element](https://www.nltk.org/book/ch01#operating-on-every-element)**\n",
    "\n",
    "This *pattern* – doing something (e.g. modifying) with every item in a sequence and returning a list of results – is called Python *list comprehension*:\n",
    "    \n",
    "`result_list = [item.do_something() for item in list]`\n",
    "\n",
    "List comprehensions may also contain conditions (only items matching the condition will be included in the resulting list):\n",
    "\n",
    "`result_list = [item.do_something() for item in list `**`if`**` condition]`\n",
    "\n",
    "It is very useful for filtering and modifying lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can filter either (a) text before calling FreqDist or (b) results of FreqDist.\n",
    "# let's filter before calling FreqDist.\n",
    "\n",
    "# create a set of stopwords (operations with sets are faster that with lists)\n",
    "stop_set = set(stop_words)\n",
    "\n",
    "# filter out stopwords (return only words not in the stoplist)\n",
    "without_stopwords = [word for word in text if word not in stop_set]\n",
    "\n",
    "text[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also filter out tokens that are not text or numbers\n",
    "\n",
    "# Python has a built-in method .isalnum() that determines \n",
    "# if a string only consists of letters or digits:\n",
    "\n",
    "# https://docs.python.org/3/library/stdtypes.html#str.isalnum\n",
    "\n",
    "filtered = [word for word in without_stopwords if word.isalnum()]\n",
    "\n",
    "filtered[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "\n",
    "freq = FreqDist(filtered)\n",
    "\n",
    "freq.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution\n",
    "freq.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data: finding interesting words\n",
    "\n",
    "NLTK also includes a list of common English words. We can use it to find unusual or mis-spelt words in a text corpus.\n",
    "\n",
    "See also: https://www.nltk.org/book/ch02#code-unusual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = nltk.corpus.words.words()\n",
    "\n",
    "# convert word list to a set (+ convert words to lowercase)\n",
    "word_set = set(word.lower() for word in word_list)\n",
    "\n",
    "# filter out common words\n",
    "uncommon = [word for word in filtered if word not in word_set]\n",
    "\n",
    "uncommon[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word frequency\n",
    "\n",
    "freq = FreqDist(uncommon)\n",
    "\n",
    "freq.most_common(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in order to find really uncommon words we may need to clean data further (convert nouns to singular, etc.) or get a larger list of common words.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further information\n",
    "\n",
    "[**Introduction to stylometry with Python**](https://programminghistorian.org/en/lessons/introduction-to-stylometry-with-python) by François Dominic Laramée\n",
    "* uses FreqDist\n",
    "\n",
    "Stylometry is the quantitative study of literary style through computational distant reading methods. It is based on the observation that authors tend to write in relatively consistent, recognizable and unique ways. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations and N-grams\n",
    "\n",
    "NLTK book: [Collocations and Bigrams](https://www.nltk.org/book/ch01#collocations-and-bigrams)\n",
    "\n",
    "**Bigrams** are just pairs of words that occur in the text.\n",
    "\n",
    "**Collocations** are sequences (e.g. pairs) of words that occur together unusually often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(nltk.bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_bigrams = nltk.bigrams(text1[:10])\n",
    "\n",
    "# to print bigrams, convert it to a list\n",
    "list(t1_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find **collocations**, we want to find bigrams that occur more often than we would expect based on the frequency of the individual words.\n",
    "\n",
    "Additional information:\n",
    "* [nltk.text.Text](https://www.nltk.org/api/nltk.html#nltk.text.Text)\n",
    "* [NLTK documentation: collocations](https://www.nltk.org/api/nltk.html#module-nltk.collocations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll_list = text1.collocation_list()\n",
    "\n",
    "coll_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also look for trigram collocations\n",
    "\n",
    "# http://www.nltk.org/howto/collocations.html\n",
    "\n",
    "coll_3 = nltk.collocations.TrigramCollocationFinder.from_words(text1)\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = coll_3.score_ngrams(trigram_measures.raw_freq)\n",
    "\n",
    "sorted(coll_3.nbest(trigram_measures.raw_freq, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in order to find unusual trigrams we would need to filter the results (and pick the most appropriate collocation measure) like it is done in `Text.collocation_list()`. \n",
    "\n",
    "Source code for `collocation_list()`: https://www.nltk.org/_modules/nltk/text.html#Text.collocation_list\n",
    "\n",
    "```\n",
    "    # print(\"Building collocations list\")\n",
    "    from nltk.corpus import stopwords\n",
    "\n",
    "    ignored_words = stopwords.words(\"english\")\n",
    "            \n",
    "    finder = BigramCollocationFinder.from_words(self.tokens, window_size)\n",
    "    finder.apply_freq_filter(2)\n",
    "    finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "            \n",
    "    bigram_measures = BigramAssocMeasures()\n",
    "    self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignored_words = stopwords.words(\"english\")\n",
    "\n",
    "coll_3 = nltk.collocations.TrigramCollocationFinder.from_words(text1)\n",
    "\n",
    "coll_3.apply_freq_filter(2)\n",
    "coll_3.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "scored = coll_3.score_ngrams(trigram_measures.raw_freq)\n",
    "\n",
    "sorted(coll_3.nbest(trigram_measures.raw_freq, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn!\n",
    "\n",
    "Choose one of NLTK corpora and **explore it using NLTK** (following examples here and in the NLTK book).\n",
    "\n",
    "Also apply what you learned (FreqDist, ...) in section \"Computing with Language: Statistics\".\n",
    "\n",
    "---\n",
    "\n",
    "**Write code in notebook cells below**.\n",
    "* add more cells (use \"+\" icon) if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
