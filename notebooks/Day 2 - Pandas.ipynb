{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abd01521",
   "metadata": {
    "id": "abd01521"
   },
   "source": [
    "# Reading Data, Pandas\n",
    "\n",
    "There are various file formats, how do we make a sense of them all?\n",
    "\n",
    "* There are archive/compression formats such as .zip, .rar, .7z, .tar those hold other files.\n",
    "* There are text formats such as .txt, .csv, .json, .tsv - those can be read by humans in a text editor\n",
    "* There are binary formats such as .exe, .jpg, .png - those are not human readable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf08d1",
   "metadata": {
    "id": "06bf08d1"
   },
   "source": [
    "### Reading text files\n",
    "\n",
    "In this section we will read a simple text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac177026",
   "metadata": {
    "id": "ac177026"
   },
   "outputs": [],
   "source": [
    "filename = \"alice_wonderland.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e88c700",
   "metadata": {
    "id": "7e88c700"
   },
   "source": [
    "The following two cells are commented out because they might not work in Google Colab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdded79",
   "metadata": {
    "id": "0bdded79"
   },
   "outputs": [],
   "source": [
    "## open the file in current directory for reading\n",
    "#file_1 = open(filename)\n",
    "\n",
    "## read contents of the file\n",
    "#data = file_1.read()\n",
    "\n",
    "## close the file\n",
    "#file_1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241a0c69",
   "metadata": {
    "id": "241a0c69"
   },
   "outputs": [],
   "source": [
    "## a better way (automatically closing the open file)\n",
    "\n",
    "#with open(filename) as file_1:\n",
    "#    data = file_1.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b67e7",
   "metadata": {
    "id": "0c5b67e7"
   },
   "source": [
    "### Google Colab\n",
    "\n",
    "Note: The above action (reading a local file) will fail if you execute it in Google Colab.\n",
    "\n",
    "We can open it from a remote web location (from Github) instead. Let's use the `requests` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db22d13",
   "metadata": {
    "id": "2db22d13"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/notebooks/\" + filename\n",
    "\n",
    "response = requests.get(url)\n",
    "data = response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1a8bd4",
   "metadata": {
    "id": "be1a8bd4"
   },
   "source": [
    "### Let's continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c83bfd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "56c83bfd",
    "outputId": "756fcba7-c03b-4cea-9fe1-5d149a8a091a"
   },
   "outputs": [],
   "source": [
    "# print the first 100 characters of the file\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b91449",
   "metadata": {
    "id": "30b91449"
   },
   "outputs": [],
   "source": [
    "# split text into tokens (words)\n",
    "words = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a4c041",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e6a4c041",
    "outputId": "0e457794-bbd3-448c-d15a-48bda830e556"
   },
   "outputs": [],
   "source": [
    "# count the number of tokens in text\n",
    "\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696dd23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6696dd23",
    "outputId": "bec01077-c2d5-48bc-a993-ca949973eccb"
   },
   "outputs": [],
   "source": [
    "# print the first 50 tokens\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f41a77",
   "metadata": {
    "id": "63f41a77"
   },
   "source": [
    "### Counting word frequency\n",
    "\n",
    "Here we will use Python's Counter object (from Python collections library) to determine word frequency of the text.\n",
    "\n",
    "https://docs.python.org/3/library/collections.html#collections.Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3391228",
   "metadata": {
    "id": "e3391228"
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac286d47",
   "metadata": {
    "id": "ac286d47"
   },
   "outputs": [],
   "source": [
    "c = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b41935a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2b41935a",
    "outputId": "e88761d3-8a97-4899-8a8f-6b2cae8c1644"
   },
   "outputs": [],
   "source": [
    "# print the 20 most common words (tokens)\n",
    "print(c.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de34be2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "de34be2e",
    "outputId": "4e89d788-6561-45bf-bf76-19c613880e54"
   },
   "outputs": [],
   "source": [
    "# a nicer way of printing counter results using a *for* cycle\n",
    "\n",
    "for token, count in c.most_common(20):\n",
    "    print(f\"{token}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecf4f58",
   "metadata": {
    "id": "5ecf4f58"
   },
   "source": [
    "Notice how words may appear in both lowercase (\"the\") and uppercase (\"The\"). You may want to normalize the text by converting it all to lowercase and do other clean-up steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5005661",
   "metadata": {
    "id": "d5005661"
   },
   "source": [
    "### Pandas\n",
    "\n",
    "Pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool, built on top of the Python programming language.\n",
    "\n",
    "https://pandas.pydata.org/\n",
    "\n",
    "Pandas lets us define `DataFrames` that contain tabular data organized in columns and rows:\n",
    "* both columns and rows may have labels (names for these columns / rows)\n",
    "* every column has its data type (different columns may have different data types)\n",
    "\n",
    "Pandas also lets us define `Series` that contain a series of data (one column). Every `Series` element may have a label (name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49d1d0b",
   "metadata": {
    "id": "f49d1d0b"
   },
   "source": [
    "### Reading TSV files\n",
    "\n",
    "Corpora that we could work with are located in archived TSV (Tab-separated-values) files:\n",
    "https://github.com/CaptSolo/BSSDH_2023_beginners/tree/main/corpora\n",
    "\n",
    "These files consist of rows (records) that contain one or more values separated by \"Tab\" characters.\n",
    "\n",
    "We will use Pandas library to read a TSV file that contains a smaller version of the \"lv_old_newspapers.zip\" corpus: https://github.com/CaptSolo/BSSDH_2023_beginners/blob/main/corpora/lv_old_newspapers_5k.tsv\n",
    "\n",
    "You may also use a TSV file for an English newspaper corpus (with slightly different column names): https://github.com/CaptSolo/BSSDH_2023_beginners/blob/main/corpora/en_old_newspapers_5k.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194e455",
   "metadata": {
    "id": "5194e455"
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "# common alternative\n",
    "# import pandas as pd\n",
    "# this would let you save 4 characters each time you need some pandas functionality you would write pd instead of pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b4e8f",
   "metadata": {
    "id": "787b4e8f"
   },
   "outputs": [],
   "source": [
    "# Commented out code that will not work in Google Colab\n",
    "\n",
    "## if you downloaded and unarchived the whole Github repository\n",
    "## this is where you will find the lv_old_newspapers_5k.tsv file:\n",
    "\n",
    "#filename = \"../corpora/lv_old_newspapers_5k.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694238e0",
   "metadata": {
    "id": "694238e0"
   },
   "outputs": [],
   "source": [
    "## read the tab-separated file (\"sep\" parameter tells Pandas that values in the file\n",
    "## are separated with the \"tab\" character.\n",
    "\n",
    "#df_1 = pandas.read_csv(filename, sep=\"\\t\") # instead of df_1 we could use another name for our variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89655e67",
   "metadata": {
    "id": "89655e67"
   },
   "source": [
    "#### Google Colab\n",
    "\n",
    "Note: The above action (reading a local file) will fail if you execute it in Google Colab.\n",
    "\n",
    "We have two different approaches then:\n",
    "\n",
    "1. Upload file to Google Colab (remember this is temporary). Read it just like you would on a local computer.\n",
    "\n",
    "2. Download file(s) from web address, instead of file path we will use its web addrss (URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q-lAt9XM54mF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "id": "q-lAt9XM54mF",
    "outputId": "067eb524-b80d-4776-cc16-15b0cabb744b"
   },
   "outputs": [],
   "source": [
    "# Approach 1\n",
    "# Assuming file has been uploaded it will be found in current directory\n",
    "\n",
    "file_path = \"lv_old_newspapers_5k.tsv\"\n",
    "\n",
    "df_1 = pandas.read_csv(file_path, sep=\"\\t\")\n",
    "\n",
    "# print the first lines of the file\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69605e00",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "69605e00",
    "outputId": "c4437db1-1fc4-4337-ed10-5e5d2c35e024"
   },
   "outputs": [],
   "source": [
    "# Approach 2 reading from a web address\n",
    "url = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/lv_old_newspapers_5k.tsv\"\n",
    "\n",
    "# ... or you could use the English corpus instead:\n",
    "# url = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/en_old_newspapers_5k.tsv\"\n",
    "\n",
    "df_2 = pandas.read_csv(url, sep=\"\\t\")\n",
    "\n",
    "# print the first lines of the file\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37da982b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "37da982b",
    "outputId": "3cbf61ef-879e-4ddf-f072-831effc379e4"
   },
   "outputs": [],
   "source": [
    "# get the basic statistics of the dataset\n",
    "df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6aaa5",
   "metadata": {
    "id": "09d6aaa5"
   },
   "source": [
    "### Let's continue working with the dataframe (containing a text corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcc2734",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bcc2734",
    "outputId": "93e16ebe-1f65-4d4b-c5c4-5e55382b5e0a"
   },
   "outputs": [],
   "source": [
    "df_1 = df_2\n",
    "\n",
    "# the size of the corpus:\n",
    "print(len(df_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c5f4f8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "62c5f4f8",
    "outputId": "17ada702-8554-4bb2-ef92-77e9f8343aff"
   },
   "outputs": [],
   "source": [
    "# select the Text column, show the first 10 entries\n",
    "\n",
    "df_1[\"Text\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eBJq74i47f-N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBJq74i47f-N",
    "outputId": "36707b18-e6c9-484d-d270-fbcda100bc06"
   },
   "outputs": [],
   "source": [
    "# we can get ALL of the text in one big string from a pandas column\n",
    "\n",
    "list_of_rows = list(df_1.Text)\n",
    "len(list_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aD-jSYH_7wMe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aD-jSYH_7wMe",
    "outputId": "f6fb2886-9a21-49b2-a637-d4f71d823fdc"
   },
   "outputs": [],
   "source": [
    "# let's see what we have in first 3 rows\n",
    "list_of_rows[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EA7Nd7sR71zq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "EA7Nd7sR71zq",
    "outputId": "0dfd5e29-9d9c-4fac-a8a3-9ad5e0b2a636"
   },
   "outputs": [],
   "source": [
    "all_text = \"\\n\".join(list_of_rows) # we can join all rows into one big string\n",
    "# separating each document with a newline, but you could choose something else to join with\n",
    "\n",
    "# \"\\n\" means a newline symbol\n",
    "\n",
    "all_text[:250]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28c46d3",
   "metadata": {
    "id": "e28c46d3"
   },
   "source": [
    "### Reading archived files\n",
    "\n",
    "Pandas can also read archived CSV and TSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0af606",
   "metadata": {
    "id": "3d0af606"
   },
   "outputs": [],
   "source": [
    "# filename_2 = \"../corpora/lv_old_newspapers.zip\"\n",
    "\n",
    "## read the archived, tab-separated file (\"compression\" parameter tells\n",
    "## Pandas that this is a ZIP archived file).\n",
    "\n",
    "# df_2 = pandas.read_csv(filename_2, sep=\"\\t\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589c131a",
   "metadata": {
    "id": "589c131a"
   },
   "source": [
    "Note: The above action (reading a local file) that is commented out will fail if you execute it in Google Colab.\n",
    "\n",
    "We will use downloading from a remote web location instead (a Github repository in this case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccef3e2",
   "metadata": {
    "id": "cccef3e2"
   },
   "outputs": [],
   "source": [
    "url_2 = \"https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/lv_old_newspapers.zip\"\n",
    "\n",
    "df_2 = pandas.read_csv(url_2, sep=\"\\t\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9587588",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e9587588",
    "outputId": "0029791e-ec90-4005-8143-447029cb3a19"
   },
   "outputs": [],
   "source": [
    "# the size of the corpus:\n",
    "\n",
    "print(len(df_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525f55f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "6525f55f",
    "outputId": "90ff4333-af5a-4f99-f248-0b012dfe8fc1"
   },
   "outputs": [],
   "source": [
    "# show the last 10 entries\n",
    "\n",
    "df_2.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6d8e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f0e6d8e9",
    "outputId": "a1035bed-531f-420d-a9c4-56afbc3c5121"
   },
   "outputs": [],
   "source": [
    "# Sorting the dataset\n",
    "df_2.sort_values(by=[\"Date\"])\n",
    "\n",
    "# Minimum value\n",
    "df_2.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0822def",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0822def",
    "outputId": "dc7cc649-73b4-4daa-f5b7-5b47836f3597"
   },
   "outputs": [],
   "source": [
    "# Maximum value\n",
    "df_2.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_HsCDJ8i8gZc",
   "metadata": {
    "id": "_HsCDJ8i8gZc"
   },
   "source": [
    "##  Reading other formats\n",
    "\n",
    "Pandas supports a wide variety of file formats\n",
    "\n",
    "Full list of formats is available here: https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html\n",
    "\n",
    "For example to read Excel files you would use my_dataframe = pandas.read_excel(filepath)\n",
    "where filepath would be a string with file location or web address"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Q4zGUWlX9TvF",
   "metadata": {
    "id": "Q4zGUWlX9TvF"
   },
   "source": [
    "## Task - read data into a dataframe from file\n",
    "\n",
    "We have 4 different corpora for you to use.\n",
    "\n",
    "Web addresses:\n",
    "\n",
    "* English - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/en_old_newspapers_5k.tsv\n",
    "* Estonian - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/ee_old_newspapers.zip\n",
    "* Latvian - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/lv_old_newspapers.zip\n",
    "* Ukrainian - https://raw.githubusercontent.com/CaptSolo/BSSDH_2023_beginners/main/corpora/ua_old_newspapers.zip\n",
    "\n",
    "Load one of them in a Pandas dataframe. Check the length, shape, sort them, see the first 15 entries and the last 20 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af637a06",
   "metadata": {
    "id": "af637a06"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bafb665",
   "metadata": {
    "id": "8bafb665"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95dfc9fd",
   "metadata": {
    "id": "95dfc9fd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1cb4fb1",
   "metadata": {
    "id": "f1cb4fb1"
   },
   "source": [
    "# Text Mining with NLTK and Pandas\n",
    "\n",
    "Source: [Text Mining and Sentiment Analysis with NLTK and Pandas in Python](https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/)\n",
    "* by Jan Kirenz\n",
    "* license: [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297c43fe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "297c43fe",
    "outputId": "7d80131b-4de6-4070-b69f-0136b3fab53e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Import some tweets from Barack Obama\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/kirenz/twitter-tweepy/main/tweets-obama.csv\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449a1a6b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "449a1a6b",
    "outputId": "f3baa92a-c9ff-4912-9a87-00587d6f2657"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ae59d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "f19ae59d",
    "outputId": "752a0fd3-e339-418d-a8f2-bb62e40ff4f1"
   },
   "outputs": [],
   "source": [
    "# Convert text to lowercase\n",
    "\n",
    "df['text'] = df['text'].astype(str).str.lower()\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee1877",
   "metadata": {
    "id": "f4ee1877"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "* We use NLTK's RegexpTokenizer to perform tokenization in combination with regular expressions\n",
    "  * `\\w+` matches Unicode word characters with one or more occurrences\n",
    "  * this includes most characters that can be part of a word in any language, as well as numbers and the underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb316ffc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "bb316ffc",
    "outputId": "9b5cf426-5737-4751-a02b-30d58adde331"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "regexp = RegexpTokenizer('\\w+')\n",
    "\n",
    "df['text_token']=df['text'].apply(regexp.tokenize)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17440f53",
   "metadata": {
    "id": "17440f53"
   },
   "source": [
    "### Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5efe59",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7d5efe59",
    "outputId": "4892f5e0-5ce0-4b6c-93aa-d150b74d7f4b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# Extend the list with your own custom stopwords\n",
    "my_stopwords = ['https']\n",
    "stopwords.extend(my_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7346defd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "7346defd",
    "outputId": "47b16960-9172-44a1-bcc5-29e1cea58dfd"
   },
   "outputs": [],
   "source": [
    "# let's create a function to remove stopwords\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    return [item for item in words if item not in stopwords]\n",
    "\n",
    "# apply this function to every dataframe row:\n",
    "df['text_token'] = df['text_token'].apply(remove_stopwords)\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af24c67f",
   "metadata": {
    "id": "af24c67f"
   },
   "source": [
    "### Remove infrequent words\n",
    "\n",
    "We first change the format of text_token to strings and keep only words which are longer than 2 letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04493f1",
   "metadata": {
    "id": "d04493f1"
   },
   "outputs": [],
   "source": [
    "# function for joining together words longer than 2 letters\n",
    "def join_words(words):\n",
    "    return ' '.join([item for item in words if len(item)>2])\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df['text_string'] = df['text_token'].apply(join_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75ecdd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "f75ecdd1",
    "outputId": "751c7b66-d0ff-4b01-d487-1fe34d37a434"
   },
   "outputs": [],
   "source": [
    "df[['text', 'text_token', 'text_string']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e212f8",
   "metadata": {
    "id": "f6e212f8"
   },
   "source": [
    "### Continue working with the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4bad10",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4f4bad10",
    "outputId": "83456084-cacd-4a96-c359-0774d93a73c0"
   },
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "\n",
    "# Create a list of all words\n",
    "all_words = ' '.join([word for word in df['text_string']])\n",
    "\n",
    "# Tokenize all_words\n",
    "tokenized_words = nltk.tokenize.word_tokenize(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GVWkdP9G3wmF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVWkdP9G3wmF",
    "outputId": "f6e0bf97-2288-4f3b-fe85-16471e62210f"
   },
   "outputs": [],
   "source": [
    "tokenized_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hM5K31s_4Ahf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hM5K31s_4Ahf",
    "outputId": "0557fb09-5f3f-4dde-a5ef-9a0dc7c8aabc"
   },
   "outputs": [],
   "source": [
    "df['text_string'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05e7dee",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d05e7dee",
    "outputId": "2e32458f-3a36-4bd0-f4a4-bacca2abda9d"
   },
   "outputs": [],
   "source": [
    "# Create a frequency distribution\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "fdist = FreqDist(tokenized_words)\n",
    "fdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05e65b5",
   "metadata": {
    "id": "a05e65b5"
   },
   "outputs": [],
   "source": [
    "# this function returns words that appear more than once\n",
    "\n",
    "def freq_words(words):\n",
    "    return ' '.join([item for item in words if fdist[item] > 1 ])\n",
    "\n",
    "# apply the function to the dataframe\n",
    "df['text_string_fdist'] = df['text_token'].apply(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fd8045",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "d5fd8045",
    "outputId": "80cc81ae-2414-430f-e9c8-b3b7000a8b83"
   },
   "outputs": [],
   "source": [
    "df[['text', 'text_string', 'text_string_fdist']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l-u5XVJC4vSD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "l-u5XVJC4vSD",
    "outputId": "a81e14e1-9568-45ed-8903-06a8f2fb414c"
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ddc0f0",
   "metadata": {
    "id": "13ddc0f0"
   },
   "source": [
    "### Summary\n",
    "\n",
    "We used Pandas to hold data of a Tweet message corpus as it went through transformations: tokenization, stopword removal, etc.\n",
    "* after each transformation we added a new Pandas dataframe column to hold the transformed data\n",
    "\n",
    "Further steps:\n",
    "* see how to visualize data in the `Day 2 - Visualization` Jupyter notebook;\n",
    "* see the [Text Mining and Sentiment Analysis with NLTK and Pandas in Python](https://www.kirenz.com/post/2021-12-11-text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/text-mining-and-sentiment-analysis-with-nltk-and-pandas-in-python/) post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ae6fda",
   "metadata": {
    "id": "23ae6fda"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a24ea476",
   "metadata": {
    "id": "a24ea476"
   },
   "source": [
    "---\n",
    "\n",
    "This notebook by Uldis BojƒÅrs is available under the [CC-BY-SA](https://creativecommons.org/licenses/by-sa/4.0/) license."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
